%!TEX root = ../main.tex

\chapter{Application of the Model}\label{cha:applying-model}

In this Chapter a method for assessing the sensitivity of the model will be outlined.
This is done by making several small changes to the parameters of strategies and observing whether the model can discriminate between the new altered vectors.
The size of the change made to the strategy will be varied and the model's performance recorded for each deviation.
It is expected that there is a cut-off where the model changes from performing well to performing badly, and this gives an approximation of its sensitivity.
Finally, the model will be applied to the Axelrod-Python library to determine if it contains any duplicate strategies.



\section{Memory One Strategies}

A memory one strategy bases its decision entirely on the the last round of play.
They are defined by a vector $v$ of length 4 that determines the probability of Cooperation given the previous rounds history \cite{Akin2015}, ie:
$$
v = (P(C|CC), P(C|CD), P(C|DC), P(C|DD))
$$
For example, a memory one strategy defined by the vector $v=(1, 1, 1, 1)$ is equivalent to Cooperator.
TitForTat can be implemented as a memory one player with the vector $v=(1, 0, 1, 0)$, Pavlov as the memory one player with $v=(1, 0, 0, 1)$ and Random as the memory one player with $v=(0.5, 0.5, 0.5, 0.5)$.
See Section \ref{sec:individual_strategies} for explanations of how these strategies operate.



\section{Determining the sensitivity of the Model}

At this point there is no measure of the sensitivity of the model.
Alternatively, there is no understanding of how similar two strategies must be before the model decides they are identical.
In this Section a method of obtaining this information using memory one strategies will be described.

All possible memory one strategies are elements of the set $S = [0, 1]^4_{\mathbb{R}}$.
For a vector $s \in S$ and a deviation $\delta \in (0, 1)$, 16 new deviated vectors are created by taking the sum of $s$ with all possible vectors $(\pm \delta, \pm \delta, \pm \delta, \pm \delta)$, but bounding each element in the range $[0, 1]$.
For example, with $s = (0.2, 0, 0.5, 0.95)$ and $\delta = 0.1$, the 16 vectors produced are:

\begin{itemize}
\begin{multicols}{4}
  \item $(0.3, 0.1, 0.6, 1.0)$
  \item $(0.3, 0.1, 0.6, 0.85)$
  \item $(0.3, 0.1, 0.4, 1.0)$
  \item $(0.3, 0.1, 0.4, 0.85)$
  \item $(0.3, 0.0, 0.6, 1.0)$
  \item $(0.3, 0.0, 0.6, 0.85)$
  \item $(0.3, 0.0, 0.4, 1.0)$
  \item $(0.3, 0.0, 0.4, 0.85)$
  \item $(0.1, 0.1, 0.6, 1.0)$
  \item $(0.1, 0.1, 0.6, 0.85)$
  \item $(0.1, 0.1, 0.4, 1.0)$
  \item $(0.1, 0.1, 0.4, 0.85)$
  \item $(0.1, 0.0, 0.6, 1.0)$
  \item $(0.1, 0.0, 0.6, 0.85)$
  \item $(0.1, 0.0, 0.4, 1.0)$
  \item $(0.1, 0.0, 0.4, 0.85)$
\end{multicols}
\end{itemize}

The 16 deviated vectors are then entered into an Ashlock tournament to produce the corresponding results table where all strategies are compared (as described in Section \ref{sec:ashlock_tourn}).
The model obtained in Chapter \ref{cha:machinelearning} is then used on these strategies to identify the proportion of correct predictions.
As all the strategies have been defined by unique vectors, it is known that all the strategies are different and so an ideal model would predict that they are all different.
By repeating this process for different vectors $s \in S$ whilst also varying $\delta$, Figure \ref{fig:m1_correct} is produced.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.9\textwidth]{../img/ML/proportion-correct.png}
    \caption{Performance of model for increasing $\delta$}
    \label{fig:m1_correct}
\end{figure}

The model appears to reach a reasonable level of accuracy when $\delta > 0.3$.
However, there is a large amount of variance for all $\delta$ used.
This suggests that the original vector has a significant effect on how the model performs.
For some memory one strategies even very small deviations will be noticed by the model, but for other memory one strategies it takes a much larger change before the model will recognise the difference.

This statistical approach can be quite precise which is more helpful than the generic graphical approach of Ashlock.
As expected the model says that the strategies are the same when there are very similar, but will otherwise easily recognise that they're different.



\section{Applying Model to Axelrod-Python}

The final section of this report addresses the problem of determining when strategies in Axelrod-Python are identical.
The assumption made in Section \ref{sec:training_model} must now be ignored.
It was assumed that \textit{Strategies with different names are assumed to be different}, but this is ultimately not known, whilst all the implementations are different it is possible that there are equivalence of strategies.

In order to determine whether any strategies in Axelrod-Python were identical, all of them were entered into an Ashlock Tournament.
The corresponding results table where all strategies are compared (as described in Section \ref{sec:ashlock_tourn}) was then computed, and the model was applied to this.
A new table could then be produced containing pairs of strategies that the model predicted were equivalent.

In Figure \ref{fig:matrix_similarity}, this table is presented as a Matrix Plot.
Note that the leading diagonal represents strategies being equivalent to themselves, also the plot is symmetric, because if strategy $A$ is similar to strategy $B$ then strategy $B$ is similar to strategy $A$.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\linewidth]{../img/ML/similarity_heatmap.png}
    \caption{Matrix Plot of similarity}
    \label{fig:matrix_similarity}
\end{figure}

However this plot does not provide much insight.
Instead it can be presented as Network Graph, as in Figure \ref{fig:overall_neighbourhoods}.
Here nodes represent strategies, and edges link nodes that the model predicted to be equivalent.
The graph is undirected for the same reason that Figure \ref{fig:matrix_similarity} is symmetric.
For clarity, strategies that are only similar to themselves have been omitted.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\linewidth]{../img/neighbourhoods/overall.png}
    \caption{Network Graph of strategies that the model had deemed equivalent}
    \label{fig:overall_neighbourhoods}
\end{figure}

In Figure \ref{fig:sep_neighbourhoods}, all of the disjoint neighbourhoods from Figure \ref{fig:overall_neighbourhoods} have been separated  and some of these will now be explored.
Figures \ref{fig:nbh_soft_gbm} and \ref{fig:nbh_hard_gbm} are perfect examples of the model exactly as expected.
Figure \ref{fig:nbh_soft_gbm} shows that the model predicted several SoftGoByMajority strategies to be equivalent to each other.
The only difference between them was the memory depth.
Again in Figure \ref{fig:nbh_hard_gbm} the only difference between the various HardGoByMajority strategies was the memory depth.
However, the model was able to distinguish between all Hard and Soft version of GoByMajority, hence the two networks are disjoint.
This suggests that changing the memory depth does not significantly change the way the strategy plays, but playing hard/soft has a large impact.

In Figure \ref{fig:nbh_alt_cd} the strategies Alternator and Cycler(DC) are deemed equivalent.
Again this is understandable as the strategies are identical except that one begins with a Cooperation and the other begins with a Defection.


\begin{figure}[htbp!]
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/1.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/10.png}\label{fig:nbh_soft_gbm}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/102.png}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/107.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/12.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/16.png}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/17.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/18.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/2.png}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/22.png}\label{fig:nbh_qlearner}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/24.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/29.png}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/30.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/31.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/36.png}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/38.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/4.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/47.png}\label{fig:nbh_alt_cd}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/49.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/55.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/58.png}\label{fig:nbh_pi_e}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/60.png}\label{fig:nbh_hard_gbm}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/7.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/74.png}} \\
\phantomcaption
\end{figure}
\begin{figure}[htbp!]
\ContinuedFloat
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/78.png}}
\subfloat[]{\includegraphics[width = 0.3\textwidth]{../img/neighbourhoods/84.png}}\\

\caption{All the disjoint neighbourhoods from Figure \ref{fig:overall_neighbourhoods}}
\label{fig:sep_neighbourhoods}
\end{figure}


In Figure \ref{fig:nbh_pi_e}, the strategies $\pi$ and $e$ are connected.
These operate by attempting to make the ratio between Cooperations and Defections as close as possible to their respective mathematical constant.
Interestingly, there is a third strategy of this form implemented in Axelrod-Python, $\phi$.
The screen shot in Figure \ref{fig:ss_pi_phi_e} shows the documentation for these strategies, including an approximation of the ratio they attempt to replicate.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width = 0.8\linewidth]{../img/screenshots/pi_phi_e.png}
    \caption{Axelrod-Python documentation for $\phi, \pi \text{ and } e$}
    \label{fig:ss_pi_phi_e}
\end{figure}

Another neighbourhood that requires more inspection is shown in Figure \ref{fig:nbh_qlearner}.
4 different QLearner strategies and the strategy AntiCycler form a complete graph, ie. the model predicts they are all equivalent to each other.
In addition, the strategy WorseAndWorse is deemed similar to AntiCycler, but not to any of the QLearners.

AntiCycler follows a sequence of plays that has no cycles: $CDD\ CD\ CCD\ CCCD\ CCCCD \dots $, whereas the QLearners use Q-Learning \cite{Watkins1992} in an attempt to find an optimal solution.
It is understandable that different Q-Learners may be predicted equivalent by the model, as it is possible that find the same solution.
However, without knowledge of the solution they find it is unclear why they are considered similar to AntiCycler and this would require further work.



\section{Conclusion}

Initially an estimation of the sensitivity of the model was obtained using memory one strategies and it was found the model achieved good results when the deviation was $\delta > 0.3$.
Then the model was applied to Axelrod-Python to determine whether the library contained any duplicate strategies.
Overall the model found 124 different neighbourhoods.
98 neighbourhoods contained only 1 strategy, ie. the strategies were unique.
15 neighbourhoods contained 2 strategies, 4 contained 3 strategies, 1 neighbourhood had 4 strategies, 2 neighbourhoods had 5 strategies, 2 had 6, 1 neighbourhood contained 7 strategies and the largest neighbourhood contained 8.

Many of the neighbourhoods contained strategies that were logical for the model to identify as identical.
The remaining neighbourhoods will require more analysis to determine whether the strategies they contain are actually equivalent or not.
One approach could be to utilise the Ashlock Fingerprints described in Chapters \ref{cha:theory} and \ref{cha:implementation}.
